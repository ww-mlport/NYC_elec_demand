---
title: "NYC Electrcity Demand Prediction - ML Model Comparison"
---
## Description

### Aim:
Compare performance of ML prediction models for electricity demand in New York City based on weather, time and date variables.

### Independent variables:
Weather (dewpoint, dry and wet bulb temperature, precipitation, humidity, pressure, visibility, wind speed), weekday/weekend and time (hourly)

### Models:
Linear regression (LR), decision tree (DT), random forest (RF), extreme gradient boosting (xgboost). 
(Models were chosen according to literature review)

### Evaluation metrics:
mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), R-squared (R2)

### Results:
#### Most important variables:
All models specified hour, temperature and weekday/weekend to be the best predictor variables.
LR - hour, dry, wet, dew bulb temp, humidity, weekday/weekend, seasons.
DT - hour, dry, wet, dew bulb temp, season, weekday/weekend.
RF - hour, dry, dew and wet bulb temp, weekday/weekend, season.
Xgb - hour, wet and dry temp, weekday/weekend

#### Model evaluation

LR is by far the worst predictor. Moderate results for DT. Both RF and xgboost performed similarly, achieving R2 values ~88% on test sets.
However, it is likely that RF and xgboost models are overfitting to training data. The R2 scores for both were near 98-99% on training data. A regularisation term, feature extraction (such as PCA) or bagging methods could help to reduce the overfitting in future analysis.


```{r include=FALSE}
library(tidyverse)
library(lubridate)
library(rpart)
library(rpart.plot)
library(gbm)
library(dplyr)
library(randomForest)
library(xgboost)
library(caret)

master <- read.csv("master.csv") %>% select(-X)

```

## 1. Prepare data for modelling

(see data_wrangling.html for detailed data prep)

Split into test and train.

Select three years for train (2016, 2017, 2018) and one for test (2019)

```{r}
#split data into test and train
train <- master %>%
  filter(year(date) %in% c(2016, 2017, 2018)) %>% 
  select(-date)
test <- master %>% 
  filter(year(date) == 2019) %>% 
  select(-date)
```

## 2. Model Building - LR, DT, RF

```{r}
# Linear Regression
lr <- lm(MWh ~ ., data = train)

# Decision Tree
dt <- rpart(MWh ~ ., data = train, 
            minsplit = 10,
            minbucket = 2,
            cp = 0.01)
rpart.plot(dt)

# Random Forest
rf <- randomForest(MWh ~ ., data = train
                   , ntree = 110)
#generate summaries
summary(lr)
summary(dt)
importance(rf)

```
### XGBoost - using random search to optimise parameters

```{r}

train_season <- train %>% select(season)
encoded_matrix_season <- as.data.frame(model.matrix(~ 0 + ., data = train_season))
train_weekdayorweekend <- train %>% select(weekdayorweekend)
encoded_matrix_weekdayorweekend <- as.data.frame(model.matrix(~ 0 + ., data = train_weekdayorweekend))
train_encoded <- cbind(train, encoded_matrix_season, encoded_matrix_weekdayorweekend) %>% select(-weekdayorweekend,-season)

n_samples <- 10

param_grid <- data.frame(
  nrounds = sample(100:300, n_samples, replace = TRUE),
  max_depth = sample(3:10, n_samples, replace = TRUE),
  eta = runif(n_samples, 0.01, 0.3),
  gamma = runif(n_samples, 0, 2),
  colsample_bytree = runif(n_samples, 0.7, 0.9),
  subsample = runif(n_samples, 0.7, 0.9),
  min_child_weight = sample(1:5, n_samples, replace = TRUE)
)

# Perform cross-validation with grid search
set.seed(123)
ctrl <- trainControl(method = "cv", number = 5)

# Define the model
model <- train(
  x = train_encoded[, -10],                      
  y = train_encoded[, 10],                        
  method = "xgbTree",                   
  trControl = ctrl,                     
  tuneGrid = param_grid,                
  metric = "RMSE"                       
)

# Print the best model and its parameters
best_model <- model$bestTune
print(best_model)

xgb <- xgboost(
  data = as.matrix(train_encoded[, -10]),         
  label = train_encoded[, 10],                    
  nrounds = best_model$nrounds,
  max_depth = best_model$max_depth,
  eta = best_model$eta,
  gamma = best_model$gamma,
  colsample_bytree = best_model$colsample_bytree,
  subsample = best_model$subsample,
  min_child_weight = best_model$min_child_weight
)
xgb_var_info <- xgb.importance(model = xgb)
print(xgb_var_info)

```

## 3. Evaluate model performance on training sets

```{r}
train_predictions <- train %>% select(MWh)
models <- list(lr, dt, rf)

for (i in 1:length(models)) {
  predictions <- as.data.frame(predict(models[[i]], newdata = train))
  model_name <- c("LR", "DT", "RF")[i]
  names(predictions)[1] <- paste(model_name,"train",sep = "_")
  train_predictions <- cbind(train_predictions, predictions)
}

xgb_train_predictions <- as.data.frame(predict(xgb, as.matrix(train_encoded[, -10])))
names(xgb_train_predictions)[1] <- "XGB_train"
train_predictions <- cbind(train_predictions, xgb_train_predictions)

# Extract true values and model estimates
true_values <- train_predictions[, 1]
model_estimates <- train_predictions[, 2:5]

# Create data frame to store results
evaluation_results <- data.frame(Model = character(0), MAE = numeric(0), MSE = numeric(0), RMSE = numeric(0), R2 = numeric(0))

# List of model names
model_names <- c("linear_regression", "decision_tree", "random_forest", "XGBoost")

# Loop through the models and calculate evaluation metrics
for (i in 1:length(model_names)) {
  model_name <- model_names[i]
  predictions <- model_estimates[, i]
  
  # Calculate evaluation metrics
  mae <- mean(abs(predictions - true_values))
  mse <- mean((predictions - true_values)^2)
  rmse <- sqrt(mse)
  r_squared <- 1 - (sum((predictions - true_values)^2) / sum((mean(true_values) - true_values)^2))
  
  # Add results to the data frame
  result_row <- data.frame(Model = model_name, MAE = mae, MSE = mse, RMSE = rmse, R2 = r_squared)
  evaluation_results <- rbind(evaluation_results, result_row)
}


print(evaluation_results)

```

### Plot training results

```{r}
library(ggplot2)
plot <- master %>%
  filter(year(date) %in% c(2016, 2017, 2018)) %>% 
  select(date, hour)
plot <- cbind(plot, train_predictions)
# Convert Date and Hour columns to a single datetime column
plot$DateTime <- ymd(plot$date) + hours(plot$hour - 1)
plot <- plot %>% select(-date,-hour)
plot <- plot %>% rename(Actual = MWh)

# Gather the Model columns into a long format for easy plotting
df_long <- tidyr::gather(plot, key = "Variable_Type", value = "Value", -DateTime)

# Create separate line plots for each Model
model_names <- unique(df_long$Variable_Type)

plots <- lapply(model_names, function(model_name) {
  if (model_name != "Actual") {
    p <- ggplot(data = df_long[df_long$Variable_Type %in% c(model_name, "Actual"), ], 
                aes(x = DateTime, y = Value, color = Variable_Type)) +
      geom_line(alpha = 0.7) +
      labs(title = paste("Comparison of Dependent Variable and", model_name),
           x = "Date and Hour",
           y = "MWh") +
      theme_minimal()
    return(p)
  }
})

# Print or save the plots
for (i in 1:length(plots)) {
  if (!is.null(plots[[i]])) {
    print(plots[[i]])
  }
}

```

## 4. Evaluate model performance on test data

```{r}

test_predictions <- test %>% select(MWh)
models <- list(lr, dt, rf)

for (i in 1:length(models)) {
  predictions <- as.data.frame(predict(models[[i]], newdata = test))
  model_name <- c("LR", "DT", "RF")[i]
  names(predictions)[1] <- paste(model_name,"test",sep = "_")
  test_predictions <- cbind(test_predictions, predictions)
}

test_season <- test %>% select(season)
test_encoded_matrix_season <- as.data.frame(model.matrix(~ 0 + ., data = test_season))
test_weekdayorweekend <- test %>% select(weekdayorweekend)
test_encoded_matrix_weekdayorweekend <- as.data.frame(model.matrix(~ 0 + ., data = test_weekdayorweekend))
test_encoded <- cbind(test, test_encoded_matrix_season, test_encoded_matrix_weekdayorweekend) %>% select(-weekdayorweekend,-season)

xgb_test_predictions <- as.data.frame(predict(xgb, as.matrix(test_encoded[, -10])))
names(xgb_test_predictions)[1] <- "XGB_test"
test_predictions <- cbind(test_predictions, xgb_test_predictions)

# Extract true values and model estimates
true_values <- test_predictions[, 1]
model_estimates <- test_predictions[, 2:5]

# Initialize a data frame to store results
evaluation_results <- data.frame(Model = character(0), MAE = numeric(0), MSE = numeric(0), RMSE = numeric(0), R2 = numeric(0))

# List of model names
model_names <- c("linear_regression", "decision_tree", "random_forest", "XGBoost")

# Loop through the models and calculate evaluation metrics
for (i in 1:length(model_names)) {
  model_name <- model_names[i]
  predictions <- model_estimates[, i]
  
  # Calculate evaluation metrics
  mae <- mean(abs(predictions - true_values))
  mse <- mean((predictions - true_values)^2)
  rmse <- sqrt(mse)
  r_squared <- 1 - (sum((predictions - true_values)^2) / sum((mean(true_values) - true_values)^2))
  
  # Add results to the data frame
  result_row <- data.frame(Model = model_name, MAE = mae, MSE = mse, RMSE = rmse, R2 = r_squared)
  evaluation_results <- rbind(evaluation_results, result_row)
}

print(evaluation_results)

```

### Plot test results vs actual

```{r}

plot_test <- master %>%
  filter(year(date) %in% c(2019)) %>% 
  select(date, hour)
plot_test <- cbind(plot_test, test_predictions)
# Convert Date and Hour columns to a single datetime column
plot_test$DateTime <- ymd(plot_test$date) + hours(plot_test$hour - 1)
plot_test <- plot_test %>% select(-date,-hour)
plot_test <- plot_test %>% rename(Actual = MWh)

# Gather the Model columns into a long format for easy plotting
df_long <- tidyr::gather(plot_test, key = "Variable_Type", value = "Value", -DateTime)

# Create separate line plots for each Model
model_names <- unique(df_long$Variable_Type)

plots <- lapply(model_names, function(model_name) {
  if (model_name != "Actual") {
    p <- ggplot(data = df_long[df_long$Variable_Type %in% c(model_name, "Actual"), ], 
                aes(x = DateTime, y = Value, color = Variable_Type)) +
      geom_line(alpha = 0.7) +
      labs(title = paste("Comparison of Dependent Variable and", model_name),
           x = "Date and Hour",
           y = "MWh") +
      theme_minimal()
    return(p)
  }
})

# Print or save the plots
for (i in 1:length(plots)) {
  if (!is.null(plots[[i]])) {
    print(plots[[i]])
  }
}
```

